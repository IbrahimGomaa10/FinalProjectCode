{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "id": "1hF2wmRmYd0M",
        "outputId": "d66cf902-57c7-43e2-f46a-8bed74aa9d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27373, 162, 1)\n",
            "(27373, 8)\n",
            "(27373, 162, 1)\n",
            "(27373, 8)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-928892c24c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;31m# y_train=np.reshape(y_train,(341,64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;31m# model.add(layers.LSTM(8, return_sequences = True,input_shape=(162, 1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/core/reshape.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [704], output_shape = [16, 1024]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 704), dtype=float32)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "# !pip install PyYAML\n",
        "# !pip install pyyaml h5py \n",
        "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
        "import librosa\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        "\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Reshape\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to play the audio files\n",
        "from IPython.display import Audio\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "Ravdess = \"/content/drive/My Drive/Ravdess/audio_speech_actors_01-24/\"\n",
        "Crema = \"/content/drive/My Drive/CREMA-D/AudioWAV/\"\n",
        "Tess = \"/content/drive/My Drive/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
        "Savee = \"/content/drive/My Drive/Savee/ALL/\"\n",
        "\n",
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "for dir in ravdess_directory_list:\n",
        "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
        "    actor = os.listdir(Ravdess + dir)\n",
        "    for file in actor:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('-')\n",
        "        # third part in each file represents the emotion associated to that file.\n",
        "        file_emotion.append(int(part[2]))\n",
        "        file_path.append(Ravdess + dir + '/' + file)\n",
        "        \n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "\n",
        "# changing integers to actual emotions.\n",
        "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
        "Ravdess_df.head()\n",
        "\n",
        "\n",
        "\n",
        "crema_directory_list = os.listdir(Crema)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in crema_directory_list:\n",
        "    # storing file paths\n",
        "    file_path.append(Crema + file)\n",
        "    # storing file emotions\n",
        "    part=file.split('_')\n",
        "    if part[2] == 'SAD':\n",
        "        file_emotion.append('sad')\n",
        "    elif part[2] == 'ANG':\n",
        "        file_emotion.append('angry')\n",
        "    elif part[2] == 'DIS':\n",
        "        file_emotion.append('disgust')\n",
        "    elif part[2] == 'FEA':\n",
        "        file_emotion.append('fear')\n",
        "    elif part[2] == 'HAP':\n",
        "        file_emotion.append('happy')\n",
        "    elif part[2] == 'NEU':\n",
        "        file_emotion.append('neutral')\n",
        "    else:\n",
        "        file_emotion.append('Unknown')\n",
        "        \n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Crema_df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tess_directory_list = os.listdir(Tess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in tess_directory_list:\n",
        "    directories = os.listdir(Tess + dir)\n",
        "    for file in directories:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('_')[2]\n",
        "        if part=='ps':\n",
        "            file_emotion.append('surprise')\n",
        "        else:\n",
        "            file_emotion.append(part)\n",
        "        file_path.append(Tess + dir + '/' + file)\n",
        "        \n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Tess_df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "savee_directory_list = os.listdir(Savee)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in savee_directory_list:\n",
        "    file_path.append(Savee + file)\n",
        "    part = file.split('_')[1]\n",
        "    ele = part[:-6]\n",
        "    if ele=='a':\n",
        "        file_emotion.append('angry')\n",
        "    elif ele=='d':\n",
        "        file_emotion.append('disgust')\n",
        "    elif ele=='f':\n",
        "        file_emotion.append('fear')\n",
        "    elif ele=='h':\n",
        "        file_emotion.append('happy')\n",
        "    elif ele=='n':\n",
        "        file_emotion.append('neutral')\n",
        "    elif ele=='sa':\n",
        "        file_emotion.append('sad')\n",
        "    else:\n",
        "        file_emotion.append('surprise')\n",
        "        \n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Savee_df.head()\n",
        "\n",
        "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
        "data_path.to_csv(\"data_path.csv\",index=False)\n",
        "data_path.head()\n",
        "\n",
        "plt.title('Count of Emotions', size=16)\n",
        "sns.countplot(data_path.Emotions)\n",
        "plt.ylabel('Count', size=12)\n",
        "plt.xlabel('Emotions', size=12)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
        "\n",
        "# taking any example and checking for techniques.\n",
        "path = np.array(data_path.Path)[1]\n",
        "data, sample_rate = librosa.load(path)\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(data):\n",
        "    # ZCR\n",
        "    result = np.array([])\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "    result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "    # Chroma_stft\n",
        "    stft = np.abs(librosa.stft(data))\n",
        "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "    # MFCC\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "    # Root Mean Square Value\n",
        "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "    result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "    # MelSpectogram\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mel)) # stacking horizontally\n",
        "    \n",
        "    return result\n",
        "\n",
        "def get_features(path):\n",
        "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "    \n",
        "    # without augmentation\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "    \n",
        "    # data with noise\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2)) # stacking vertically\n",
        "    \n",
        "    # data with stretching and pitching\n",
        "    new_data = stretch(data)\n",
        "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "    res3 = extract_features(data_stretch_pitch)\n",
        "    result = np.vstack((result, res3)) # stacking vertically\n",
        "    \n",
        "    return result\n",
        "X, Y = [], []\n",
        "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
        "    feature = get_features(path)\n",
        "    for ele in feature:\n",
        "        X.append(ele)\n",
        "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
        "        Y.append(emotion)\n",
        "        \n",
        "Features = pd.DataFrame(X)\n",
        "Features['labels'] = Y\n",
        "Features.to_csv('features.csv', index=False)\n",
        "Features.head()\n",
        "\n",
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
        "\n",
        "# scaling our data with sklearn's Standard scaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
        "# making our data compatible to model.\n",
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
        "\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(y_train))\n",
        "\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Dense(units=8, activation='relu'))\n",
        "\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(y_train))\n",
        "\n",
        "\n",
        "# y_train=np.reshape(y_train,(341,64))\n",
        "model.add(Reshape((4*4, 1024)))\n",
        "\n",
        "# model.add(layers.LSTM(8, return_sequences = True,input_shape=(162, 1)))\n",
        "# model.add(layers.LSTM(8))\n",
        "\n",
        "\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.add(LSTM(units = 50, return_sequences = True, dropout = 0.5))\n",
        "model.add(LSTM(units = 20, return_sequences = False, dropout = 0.5))\n",
        "model.add(Dense(output_dim = 7, activation = 'softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
        "history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])\n",
        "\n",
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "epochs = [i for i in range(200)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "test_acc = history.history['val_accuracy']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "fig.set_size_inches(20,6)\n",
        "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
        "ax[0].set_title('Training & Testing Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
        "ax[1].set_title('Training & Testing Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "plt.show()\n",
        "\n",
        "pred_test = model.predict(x_test)\n",
        "y_pred = encoder.inverse_transform(pred_test)\n",
        "\n",
        "y_test = encoder.inverse_transform(y_test)\n",
        "\n",
        "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
        "df['Predicted Labels'] = y_pred.flatten()\n",
        "df['Actual Labels'] = y_test.flatten()\n",
        "\n",
        "df.head(10)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (12, 10))\n",
        "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
        "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
        "plt.title('Confusion Matrix', size=20)\n",
        "plt.xlabel('Predicted Labels', size=14)\n",
        "plt.ylabel('Actual Labels', size=14)\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "model.save(\"modelllll.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# load model\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Modelllllll",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}